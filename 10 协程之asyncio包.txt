在开始之前，再补充一下我对协程的一些理解：
协程不是计算机提供的，而是程序员认为创造的（计算机只提供线程和进程）

协程又被称为微线程，是一种用户态内的上下文切换技术(线程和进程的调度由cpu和内核决定，而协程的调度由我们开发者在用户态程序代码中决定)。简而言之就是通过一个线程实现代码块相互切换执行。

实现协程的几种方式：
1.作为生成器的协程（yield和yield from）
2.async 和 await 原生协程
3.python提供的异步io框架（asyncio）

=====================================

asyncio是用于异步io高并发编程的模块。

asyncio包含的功能：
1.asyncio包含各种特定模块(select/poll/epoll)的事件循环功能
2.传输和抽象协议
3.对TCP、UDP、SSL、子进程、延时调用以及其他的具体支持
4.模仿futures模块但适用于事件循环使用的Future类
5.基于yield from，可以让你用顺序的方式编写并发代码
6.必须使用一个将产生阻塞IO的调用时，有接口可以把这个事件转移到线程池


我们来看看asyncio的官方手册是如何描述asyncio这个包的：
asyncio 是用来编写 并发 代码的库，使用 async/await 语法。
asyncio 被用作多个提供高性能Python异步框架的基础，包括网络和网站服务，数据库连接库，分布式任务队列等等。

asyncio 往往是构建 IO 密集型和高层级 结构化 网络代码的最佳选择。

asyncio 提供一组 高层级 API 用于:
并发地 运行 Python 协程 并对其执行过程实现完全控制;
执行 网络 IO 和 IPC;
控制 子进程;
通过 队列 实现分布式任务;
同步 并发代码;

此外，还有一些 低层级 API 以支持 库和框架的开发者 实现:
创建和管理 事件循环，以提供异步 API 用于 网络化, 运行 子进程，处理 OS 信号 等等;
使用 transports 实现高效率协议;
通过 async/await 语法 桥接 基于回调的库和代码。

===================================
接下来举一个例子让我们初步认识asyncio。这个例子中我们不用具体明白每句代码是干啥用的，而是要了解协程的一个切换过程。

例子1：asyncio的简单使用，模拟io操作

# coding=utf-8

import asyncio, time

# 用asyncio.coroutine装饰器声明func函数是一个协程函数
@asyncio.coroutine
def func1():
    print(1)
    yield from asyncio.sleep(2)     # 用sleep()模拟io请求
    print(2)

@asyncio.coroutine
def func2():
    print(3)
    yield from asyncio.sleep(2)     # 用sleep()模拟io请求
    print(4)

# 封装2个协程到tasks中
tasks = [
    asyncio.ensure_future(func1()),
    asyncio.ensure_future(func2())
]
loop = asyncio.get_event_loop()     # 创建事件循环对象
loop.run_until_complete(asyncio.wait(tasks))       # 开始事件循环

当开始事件循环的时候，会先运行func1()这个协程，当遇到 yield from  asyncio.sleep(2)的时候就会自动切换到tasks中的func2()协程去运行，当func2也运行到yield from  asyncio.sleep(2)的时候，此时两个协程都处于sleep等待状态，这个时候整个线程就会发生阻塞，直到其中一个协程睡醒，就会通知事件循环loop，loop就会切换到这个协程恢复它的运行。

这就是asyncio运行协程时做的事：在协程空闲时（例如遇到IO操作），切换运行其他协程，以保证单线程一直不闲着，不是在协程A中运行就是在协程B中运行。但是如果所有协程都遇到了要等待的操作，则线程只好真的闲着了。



但是在python 3.8之后，asyncio.coroutine装饰器就已经废除，改为使用async 和 await

将上面的代码改为使用async 和 await; async 就相当于上例的@asyncio.coroutine，await相当于上例的yield from
# coding=utf-8

import asyncio, time

# 用async关键字声明func函数是一个协程函数
async def func1():
    print(1)
    await asyncio.sleep(2)     # 用sleep()模拟io请求
    print(2)

async def func2():
    print(3)
    await asyncio.sleep(2)     # 用sleep()模拟io请求
    print(4)

# 封装2个协程到tasks中
tasks = [
    asyncio.ensure_future(func1()),
    asyncio.ensure_future(func2())
]
loop = asyncio.get_event_loop()     # 创建事件循环对象
loop.run_until_complete(asyncio.wait(tasks))       # 开始事件循环

效果是一样的。


=================================

asyncio的核心是 事件循环 + 协程 

其实事件循环本质上就是一个死循环,并且在循环过程中不断检测任务是否可执行。

下面给出一段伪代码，这段伪代码描述了事件循环到底做了什么事情：

# Fake code

任务列表 = [task1, task2, ...]		# 或者叫做协程池

while True:
	可执行的任务列表, 已完成的任务列表 = 去任务列表检查所有任务()
	
	for 就绪任务 in 可执行的任务列表:
		继续执行已就绪的任务（如果是生成器协程，这里的代码就是next(gen)或者gen.send()）
		
	for 已完成的任务 in 已完成的任务列表：
		从任务列表移除 已完成的任务
		
	if len(任务列表) == 0:
		break
		
在事件循环中，io未完成的任务会被忽略，io操作完成的任务会被返回以供处理。

我们在上一节知道，生成器协程(yield)是通过next(gen)或者gen.send(None)开始运行的。
那么asyncio如何运行一个协程：只需把协程放到事件循环中即可开始运行：

例子2：开始运行一个协程

import asyncio


# 定义一个协程函数
async def func():
    print("开始运行协程")
    print("协程运行完毕")


coro = func()  # 创建一个协程，此时协程函数还不会开始运行
loop = asyncio.get_event_loop()  # 获取一个事件循环对象

loop.run_until_complete(coro)  # 将协程或者任务放到“任务列表”中，并开始事件循环


在python3.7中，asyncio.run(coro)也可以运行协程，它其实做的就是 loop = asyncio.get_event_loop() 和 loop.run_until_complete(coro)这两件事。
run方法其实也是启动了一个事件循环。

====================================

await 关键字。

await只能在async声明的函数中使用。
await后面只能接可等待对象(Awaitable对象)，在python中，可等待对象有3中：协程、Future和Task对象。总之，可以理解为await后面需要接需要io等待的操作，而且这些操作必须是Awaitable对象。

当协程执行到await的时候，就会暂停这个协程的运行，并切换到其他协程运行，直到await后面接的io操作结束才会恢复该协程的运行（这个过程由run_until_complete内部的事件循环完成，具体怎么完成的请看上面的伪代码）

例子3-1：await
# coding=utf-8

import asyncio,random,time

# 协程函数：请求一个页面
async def request_url(url):
    print("请求url %s" % url)
    await asyncio.sleep(1)      # 模拟io请求
    print("完成请求 %s" % url)

    if url.find("detail") == -1:    # 如果这个url是一个列表页则获取详情页链接
        detail_urls = ["%s/detail%s" % (url, str(random.randint(1,100))) for i in range(5)]
        return detail_urls
    else:   # 如果url不是列表页则直接返回html内容
        return url + "的html内容"

# 协程函数：爬取一个列表页下所有详情页内容
async def crawl(list_url):
    print("start crawl")

    detail_urls = await request_url(list_url)

    for detail_url in detail_urls:
        detail_html = await request_url(detail_url)
        print("详情页内容： %s" % detail_html)

st = time.time()
asyncio.run(crawl("/list1"))
print("用时: %.5f" % (time.time() - st))		# 用时: 6.04015


这个例子启动了一个协程，这个协程先请求了1个列表页，再请求从列表页获取到的5个详情页，用时大约6点几秒。5个详情页的请求是顺序执行的而非并发的。

整个运行过程如下图：


结论是：
一个协程内的多个await之间（多个request_url协程之间）是串行的而不是并发的（如果把一个协程内多个要await的Awaitable放到事件循环任务列表中，他们之间就会变为并发，学到Task对象时会讲解），必须要等上一个await中的代码执行完了才能执行下一个await。但是并不是说串行是没有意义的，当下一个io操作依赖上一个io操作的返回值时，await的这种串行功能就会起到作用。



例子3-2：并发爬取多个列表页：
# coding=utf-8

import asyncio,random,time

# 协程函数：请求一个页面
async def request_url(url):
    print("请求url %s" % url)
    await asyncio.sleep(1)      # 模拟io请求
    print("完成请求 %s" % url)

    if url.find("detail") == -1:    # 如果这个url是一个列表页则获取详情页链接
        detail_urls = ["%s/detail%s" % (url, str(random.randint(1,100))) for i in range(5)]
        return detail_urls
    else:   # 如果url不是列表页则直接返回html内容
        return url + "的html内容"

# 协程函数：爬取一个列表页下所有详情页内容
async def crawl(list_url):
    print("start crawl")

    detail_urls = await request_url(list_url)

    for detail_url in detail_urls:
        detail_html = await request_url(detail_url)
        print("详情页内容： %s" % detail_html)

st = time.time()
task_list = [crawl("/list" + str(i+1)) for i in range(10)]    # 创建10个crawl协程
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(task_list))
print("用时: %.5f" % (time.time() - st))		# 用时: 6.03965


例子3-1和例子3-2的区别是，3-1只开了一个爬取列表页的协程，而3-2开了10个列表页的协程。但是两者的事件都是6秒多。
在例子3-2中，多个crawl协程之间是并发的，多个request_url(list_url)协程之间是并发的，多个request_url(detail_url)之间是并发的。而一个crawl协程内多个request_url子协程是串行的。


介绍一个技巧，无论是在这个例子还是之后的例子，如果想具体知道整个线程在多个协程和子协程的切换顺序和代码运行的路径过程，很简单，多print然后运行一下就知道了。这样也就知道哪些协程是并发的哪些是串行的了。

======================================

Task对象

下面是官方对Task的描述：Tasks用于并发调度协程，通过asyncio.create_task(协程)的方式创建Task对象，这样可以让协程加入事件循环中等待被调度执行。
除了使用asyncio.create_task()函数之外，还可以使用低层级的loop.create_task()或ensure_future()。不建议手动实例化Task对象。
create_task()只能在python 3.7之后使用。而ensure_future()兼容以前版本。
create_task()必须在事件循环已经建立好的情况下才能调用否则会报错。
create_task()做了两件事：把协程放到事件循环的任务列表中监听；返回一个Task对象

例子4：改变例子3-1，让5个详情页的请求是并发的而不是串行的。

# coding=utf-8

import asyncio,random,time

# 协程函数：请求一个页面
async def request_url(url):
    print("请求url %s" % url)
    await asyncio.sleep(1)      # 模拟io请求
    print("完成请求 %s" % url)

    if url.find("detail") == -1:    # 如果这个url是一个列表页则获取详情页链接
        detail_urls = ["%s/detail%s" % (url, str(random.randint(1,100))) for i in range(5)]
        return detail_urls
    else:   # 如果url不是列表页则直接返回html内容
        return url + "的html内容"

# 协程函数：爬取一个列表页下所有详情页内容
async def crawl(list_url):
    print("start crawl")

    detail_urls = await request_url(list_url)

	# 创建多个Task对象，存放在列表中
    task_list = [asyncio.create_task(request_url(detail_url)) for detail_url in detail_urls]

	# 请尝试注释掉这3行代码运行，看看会发生什么
    for task in task_list:
        detail_html = await task	# task对象是可等待对象，可以放在await后面	
        print("详情页内容： %s" % detail_html)

st = time.time()
asyncio.run(crawl("/list1"))
print("用时: %.5f" % (time.time() - st))      # 用时: 2.02461

当执行create_task()的时候，协程被放到事件循环的任务列表参与被调度，这时详情页的request_url()函数内的代码就开始执行。await task会等待详情页的request_rul协程执行完才结束crawl协程。

注意：await request_url(list_url) 与 5个 await task 是串行的，而5个 await task 之间是并发的（运行到await asyncio.sleep(1)时，单线程会在5个详情页协程之间切换）。create_task()的作用就是将5个await task之间的运行关系从串行变为并发。这也是官方文档所说的：“Tasks用于并发调度协程”的真正含义。


上面的写法还可以写的更优雅一些：
# 协程函数：爬取一个列表页下所有详情页内容
async def crawl(list_url):
    print("start crawl")

    detail_urls = await request_url(list_url)

    # 创建多个Task对象，存放在列表中
    task_list = [asyncio.create_task(request_url(detail_url)) for detail_url in detail_urls]

    # 请尝试注释掉这行代码运行，看看会发生什么
    done, _ = await asyncio.wait(task_list)   # done 是一个集合封装着多个对象r,r.result就是request_url()的返回值

    [print("详情页内容： %s" % res.result) for res in done]

 
PS：create_task()必须在事件循环已经建立好的情况下才能调用否则会报错
事件循环没有启动，你往任务列表里面放Task或协程也不会被调度



总结：
1.只有放到了任务列表中的协程或任务，这些协程或任务之间是并发的，其他的任务或协程和它们之间的关系都是串行的。
2.一个协程X，如果它遇到 <await 协程Y或任务Y>这样的结构时，会往协程Y里面钻去运行协程Y内的代码（协程Y是协程X的子协程），如果协程Y中还有<await 子协程Z>，还会继续下沉，直到遇到 <await io操作或asyncio.sleep>这样的结构就会切换到其他协程或者阻塞整个线程。
3.一个协程X，如果它遇到  <await io操作或asyncio.sleep> 这样的结构时,如果此时事件循环内的任务列表中还有不处于“等待io”的状态的协程，线程就会切换到这样的协程去运行，如果任务列表中全都是处于“等待io”的状态的协程，此时整个线程都会阻塞（其实本质上是切回了主调用方执行到事件循环的select.select()方法，被这个方法阻塞，而非被asyncio.sleep阻塞，因为asyncio.sleep是个非阻塞异步方法，而且协程中也不允许使用阻塞的方法，否则调用方根本无法执行事件循环），直到有协程的io操作完成（假设是协程Y），此时线程会马上从协程X切换到协程Y。
4.在协程异步io编程中，一个单线程如果发生阻塞，这个阻塞肯定是发生调用方的事件循环的检测任务方法上(select.select())。
5.一个协程必须等所有它里面的子协程全都执行完以后才会结束无论子协程之间是否是并发的。如果一个协程内的所有子协程全都处于等待状态，那么这个父协程也会处于等待状态（这个时候线程是否阻塞就看还有没有其他父线程是就绪的了）。

还是那句话：在每个协程中多print几下，然后运行，看看print的结果就知道运行的顺序和各个协程之间的并发或串行关系了。


=======================================================================

Future对象（这里说的是asyncio中的Future）

Future类是Task类的父类，是一个更低级的可等待对象。当await一个future对象时，如果future对象中已经设置了结果result，则await不会等待，直接返回future中的result。如果future对象中没有设置结果result，则await会等待，此时要么由事件循环切换协程要么被事件循环阻塞整个线程，

例子5：Future对象
# coding=utf-8

import asyncio,random,time

# 需要传入一个future对象作为参数
async def set_after(fut):
    await asyncio.sleep(2)
    fut.set_result("666")       # 在一个协程完成之后，在future对象中设置结果

async def main():
    # 获取当前事件循环
    loop = asyncio.get_running_loop()

    # 创建一个空的Future对象
    fut = loop.create_future()

    await asyncio.create_task(set_after(fut))

    result = await fut      # 得到666

    print(result)

asyncio.run(main())

分析：main和set_after这两个协程被放到了任务列表，fut这个Future对象没放到任务列表，所以main和set_after之间是并发的，set_after和fut是串行的。因此，set_after执行到await sleep(2)时会切回main运行到await fut ,由于fut中还没设置结果，因此阻塞，整个线程也被阻塞住，2秒后，fut对象设置结果666，会返回给result，线程结束。

asyncio.Future对象 和 concurrent.futures.Future对象

前者是使用asyncio协程完成任务时会设计的对象；后者是使用线程池或者进程池完成任务时会遇到的对象。两个Future对象没有半毛钱关系，但是他们的共同点都是用于等待任务结果的返回。

以后写代码可能会出现交叉使用这两种Future对象，这意味着同时使用协程和多线程多进程开发。例如：一个项目里面都是用的协程进行异步编程，但是这个项目引入了一个第三方库如mysql数据库，它是使用的多线程异步编程，此时就会交叉使用。

如果想要结合多线程和协程的话，就要通过Future对象，下面是python官方的例子：

例子6：多线程结合协程
# coding=utf-8

import asyncio,random,time

def func():
    time.sleep(2)   # 模拟io操作
    return "666"

async def main():
    loop = asyncio.get_running_loop()   # 获取事件循环

    # run_in_executor方法做了两件事
    # 1.调用ThreadPoolExecutor的submit方法去线程池申请一个线程执行func函数，并返回一个concurrent.futures.Future对象
    # 2.调用asyncio.wrap_future将多线程的Future对象包装为asyncio的Future对象，因为asyncio的future对象才支持await语法
    fut = loop.run_in_executor(None, func)  # 该方法不阻塞，第一参传一个线程池或进程池对象，None则默认线程池
    result = await fut    # 会阻塞等待func返回666才继续
    print(result)

asyncio.run(main())


强调一下：当一些python模块不支持使用协程进行异步io编程时才会把多线程和协程混用。


例子7：混用asyncio协程 + 不支持异步的模块；以爬虫为例，下面使用requests这个同步模块写爬虫

# coding=utf-8

import asyncio,random,time,requests

# 爬一个url就开一个协程
async def getUrl(url):
    loop = asyncio.get_running_loop()

    # 启用线程池完成请求url的任务，并返回一个Future对象
    fut = loop.run_in_executor(None, requests.get, url)

    result = await fut      # 等待请求本url的这个特定线程运行完成并获取fut对象中设置的结果（requests.get的返回值）

    print(result.text)

urls = [
    "https://www.zbpblog.com/",
    "https://www.zbpblog.com/blog-196.html",
    "https://www.zbpblog.com/blog-195.html",
    "https://www.zbpblog.com/blog-194.html",
]

# 创建协程
task_list = [getUrl(url) for url in urls]

# 创建事件循环
loop = asyncio.get_event_loop()

# 将协程放入任务列表中监控
loop.run_until_complete(asyncio.wait(task_list))



这个是正确示范。这个例子中一个url对应开一个协程，一个协程中又对应开一个线程去爬url。
但是在实际应用中没有必要这样写，如果真的要用requests这个非异步模块做爬虫的话，就直接用多线程就可以了，没有必要又用多线程又用协程。
而且，如果直接使用协程会比用多线程+协程爬的效率更高，原因是线程池会限制并发的线程数，顶多就只能开十几二十个线程，此时协程就会被有限的线程数给拉低了爬取速度（短板效应）。而直接用协程的话，可以开成千上万个协程，同样是一秒，如果多线程可以爬5个url，协程就可以爬5000个url（当然啦，如果这么做的话肯定会被对方服务器给封禁IP的，而且对方服务器也不一定能并发处理这么多的连接）。




下面做一个错误示范：
例子8：错误示例
# coding=utf-8

import asyncio,random,time,requests

# 爬一个url就开一个协程
async def getUrl(url):
    result = requests.get(url)		# 加await会报错，因为requests.get()返回的不是一个Awaitable对象

    print(result.text)

urls = [
    "https://www.zbpblog.com/",
    "https://www.zbpblog.com/blog-196.html",
    "https://www.zbpblog.com/blog-195.html",
    "https://www.zbpblog.com/blog-194.html",
]

# 创建协程
task_list = [getUrl(url) for url in urls]

# 创建事件循环
loop = asyncio.get_event_loop()

# 将协程放入任务列表中监控
loop.run_until_complete(asyncio.wait(task_list))


这个实例没有报错，错就错在不能在协程中使用requests.get这个同步阻塞方法（同步就是要等操作完成才返回，异步是操作没完成也可以立刻返回，即使是返回None）。如果在协程中用阻塞的方法，就完全达不到单线程并发的效果。

========================================
使用Task对象或Future对象使调用方能得到协程的返回值

例子12：获取协程的返回值

# coding=utf-8

import asyncio

async def func(i):
    print("开始")
    await asyncio.sleep(1)
    print("结束")

    return i

loop = asyncio.get_event_loop()     # 开启事件循环必须放在create_task之前否则会报错
task_list = [loop.create_task(func(i)) for i in range(5)]
loop.run_until_complete(asyncio.wait(task_list))    # 这个方法会阻塞

print(123)

for task in task_list:
    print(task.result())    # 获取协程的返回值,这个方法会阻塞，直到协程return了才会被唤醒。但是能过得了run_until_complete这个方法的阻塞，走到这里的result()方法都不会阻塞的
	

例子13：在携程完成后调用回调函数
# coding=utf-8

import asyncio


async def func(i):
    print("开始")
    await asyncio.sleep(1)
    print("结束")

    return i

# 协程完成后会自动调用该回调函数，并且传入一个future对象
def cb(future):
    print("执行回调函数，协程返回值为： %s" % future.result())

loop = asyncio.get_event_loop()  # 开启事件循环必须放在create_task之前否则会报错
task_list = [loop.create_task(func(i)) for i in range(5)]   # 将任务放入任务列表中（或者说把协程放到协程池中调度）
for task in task_list:
    task.add_done_callback(cb)    # 让协程结束后自动调用一个回调函数
loop.run_until_complete(asyncio.wait(task_list))  # 等待协程运行，这个方法会阻塞


如果我想往cb中传入其他参数，可以使用 functools.partial(),他可以把一个函数变为另一个函数，并传入你想传的参数。

用法如下：

from functools import partial

# 其他参数要放在future参数前。
def cb(url, future):
    print("执行回调函数，协程返回值为： %s" % future.result())
	
...

task.add_done_callback(partial(cb, url))

========================================

异步迭代器（不重要）

迭代器是实现了 __iter__ 和 __next__ 魔术方法的对象。

异步迭代器是实现了 __aiter__ 和 __anext__魔术方法的对象。

__aiter__返回异步迭代器self本身
__anext__必须返回一个awaitable对象，async for 会处理异步迭代器的 __anext__()方法所返回的可等待对象。直到引发一个 StopAsyncIteration 异常。

async for 和 for 都是循环，只不过async for 只能在协程函数中使用否则会报错。

例子9：异步可迭代对象
# coding=utf-8

import asyncio,random,time,requests

class Reader:
    def __init__(self, n):
        self.maxcount = n
        self.count = 0

    async def readline(self):
        await asyncio.sleep(1)    # 模拟读取文本中的一行
        self.count += 1

        if self.count >= self.maxcount:
            return None

        return self.count

    def __aiter__(self):
        return self

    # __anext__ 必须是一个用async声明的协程函数
    async def __anext__(self):
        val = await self.readline()     # 凡是遇到阻塞的地方都要用 await 等待
        if val is None:
            raise StopAsyncIteration
        return val

# 由于async for必须在一个协程中运行，而不能在调用方运行，所以这里定义一个协程函数main
async def main():
    obj = Reader(10)
    async for line in obj:  # 遍历这个异步可迭代对象
        print(line)

asyncio.run(main())

我以为这个例子中遍历10次是并发的，结果是串行的，花了10秒的时间。

异步可迭代对象仅作了解即可，知道有这么一回事


======================================

异步上下文管理器：

定义了 __aenter() 和 __aexit__ 方法的对象就是异步上下文管理器。这两个方法都要使用async声明

异步上下文管理器支持使用 async with

首先我们要知道什么是上下文管理器，它类似于一个在执行操作之前先帮你自动创建一个资源标识或者创建连接，执行完操作之后帮你自动关闭这个连接的管理器。

__aenter__编写执行操作前的操作， __aexit__执行操作完成后的操作。


例子10：异步上下文管理器(这是一段伪代码)
# coding=utf-8

import asyncio

class AsyncContextManager:
    async def __aenter__(self):
        # 做数据库连接的操作
        # self.conn = await 数据库连接()    # 连接这个行为必须是异步的才行
        await asyncio.sleep(1)      # 用睡眠模拟连接的过程
        pass

    async def do_something(self):
        # 操作数据库
        await asyncio.sleep(1)
        return 666

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()       # 这个close方法也必须是异步非阻塞的才行

async def func():
	async with AsyncContextManager() as f:
		result = await f.do_something()		# 这里必须要await
		print(result)
		
asyncio.run(func())
	

记住，凡是执行你觉得需要等待的操作或者方法都要用await关键字。

执行到 async with 会自动调用 __aenter__ 
当跳出async with 代码块的时候会执行 __aexit__

async with 也必须在协程中使用，所以要把它放在协程函数 func 中。

在协程中，如果执行到async with ，意味着可能会等待而切换协程，对于async with 的理解，我会在介绍aiomysql的连接池的时候用例子来说明。

=================================

uvloop

这是asyncio事件循环的替代方案，uvloop是一个第三方库。asyncio本身内置的事件循环的效率是uvloop的一半。

它的使用方式非常简单:
import uvloop 

# 将asyncio内部的事件循环替换为uvloop的事件循环
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

# 之后的运行协程的方式和之前一模一样

===================================

asyncio 异步操作 redis

异步操作redis的意义何在？
例如我在A服务器请求B服务器的redis，此时连接/发送命令请求/断开连接 都是网络IO（redis执行命令很快，它的瓶颈就是网络延迟，即命令和返回结果在网络传输上花很多时间）。
如果我们用redis命令在网络中传输这段时间做点其他事情岂不快哉？

如果想结合asyncio 和 redis ，就不能使用以前的redis库了，因为redis库的方法都是同步阻塞的。

可以使用aioredis，它支持异步操作redis

例子11：简单使用aioredis

# coding=utf-8

import asyncio, aioredis

# 既然aioredis里面的方法都是异步非阻塞的，那么肯定要放在协程函数中运行才可以啦，所以定义一个协程函数execute()
async def execute(address):
    print("开始连接redis")
    redis = await aioredis.create_redis(address)    # 创建一个redis对象，这里会发生网络IO（io的意思是读和写的意思）等待（是这个协程在等，整个线程不会去等的，肯定会切换到其他线程去运行），所以要await。此时如果事件循环的任务列表中还有其他协程就可以切换到其他协程运行，而不会干等着了。

    # 下面的读写操作都会发生网络IO，都要await
    await redis.set("a", "a")

    result = await redis.get("a")

    # 关闭连接，不用await，虽然关闭连接也会进行网络IO，但是我们没有必要等，因为我们后面都用不着redis了。
    redis.close()

    # 如果需要等待连接关闭可以加上下一句
    # await redis.wait_closed()

    print("结束")

task_list = [execute("redis://127.0.0.1:6379") for i in range(5)]   # 创建5个协程
asyncio.run(asyncio.wait(task_list))    # 把这5个协程注册到事件循环中还行



类似的异步操作mysql有aiomysql库，mysql库的方法也都是同步阻塞的，所以要用aiomysql里面的方法都是异步非阻塞的，可以立刻返回。


所以如果要用异步协程编程，我们就要查看用到的库有没有异步版本的库。

==========================================================================

补充：

1.run_until_complete 和 run_forever

我们知道，当执行 get_event_loop 方法的时候会获取一个事件循环对象，但事件循环还没开始。
当执行run_until_complete 和 run_forever 时会开始事件循环。二者的区别是，前者会在所有协程结束后停止事件循环，后者不会而是一直事件循环下去。

其实，run_until_complete方法内部会把传进这个方法的协程coro给封装为Task对象，然后再放到任务列表中。


2.协程间同步和通信
是不是很奇怪。协程之间也需要同步和通信的吗？要的，举一个例子：

例子14：协程间同步和通信
import asyncio, aiohttp

cache = {}		# 缓存

# getUrl协程要请求url并且将页面内容放入到缓存中
async def getUrl(url):
	if url in cache:
		return cache[url]
		
	content = await aiohttp.request('GET', url)	# 请求url
	
	cache[url] = content	# 并放入到缓存
	return content
	
async def parseContent(url):
	content = await getUrl(url)
	# do something
	
async def useContent(url):
	content = await getUrl(url)
	# do some other thing
	
url = "http://www.baidu.com"
task_list = [parseContent(url), useContent(url)]
asyncio.run(asyncio.wait(task_list))

现在有两个协程 parseContent 和 useContent并发运行，他们里面都调用了子协程函数getUrl。在 parseContent 执行的时候，它里面执行到子协程 getUrl 的aiohttp.request()会等待，所以切换到useContent 执行，它里面的 getUrl 会对同一个url请求第二次。
这样一来多发了一次重复的请求，效率低了，二来可能会引发服务器反爬虫机制。

此时我们希望给协程 getUrl 对同一个url的爬取上一把锁。

asyncio库中也是有锁，条件变量和队列的，只不过他们都是异步的，意味着给一个getUrl协程的某块代码上锁后，当其他getUrl在运行到这行去获取锁的时候会等待锁释放才能拿到锁，但是等待锁的时候不会真的阻塞，而是切换到其他协程工作。这也是异步锁和同步锁的区别。它的原理其实是在上锁(release)的时候创建了一个需要等待的协程，所以在调用release()的时候前面要加上await()

具体代码如下：


协程的Queue队列内部是没有使用锁的，因为这是一个单线程，从Queue队列的弹出数据或放入数据都不会引起数据混乱，所以实际上协程的Queue和一个普通的列表没有什么区别。区别在于Queue可以限制长度，list不行。

===================================

最后的最后，当然是写个爬虫检验一下协程异步编程的成果啦！

这个小爬虫要做几个任务：
1.爬取多个列表页url（列表页是一个api接口返回的是包含详情页url、标题、描述和缩略图的json格式）
2.爬取列表页所显示的详情页url
3.把详情页的内容写入到mysql中
4.将详情页的图片下载下来并将对应的图片名和文章标题，内容插入到数据表的同一行

为了完成这3个任务，我要定义这么几类协程：
A.初始化生成列表页的协程（其实就是一个简单的for循环）
B.爬取并解析列表页的协程（会生成详情页的url）
C.爬取并解析详情页的协程
D.将详情页html存到mysql的协程
E.下载详情页图片的协程
F.主协程，用于把上面的协程加入任务列表（协程池）的初始化协程。

在正式实现代码之前，先介绍一下aiomysql，aiohttp和aiofiles的用法：

==================================================

aiomysql

官方文档：https://aiomysql.readthedocs.io/en/latest/
如果想看中文文档，可以参考本博客网站关于aiomysql的文章，该文章是我把官方文档翻译了之后的中文文档。

aiohttp(这里仅介绍aiohttp做客户端使用)

官方文档：https://docs.aiohttp.org/en/stable/client_quickstart.html


aiofiles


下面是代码实现：
# coding=utf-8

import asyncio, aiohttp, aiomysql, re, hashlib, os, aiofiles, time
from asyncio import Queue
from bs4 import BeautifulSoup

class Crawler:
    def __init__(self, baseUrl, dbCfg, listUrlInfo, picPath = './pic', pageRows=10, maxCoroNum = 20, tcpConnNum = 20, maxMonitorCount = 10):
        self.baseUrl = baseUrl.strip('/') + "/"
        self.picPath = picPath              # 下载图片到本地的目录
        self.queue = Queue(maxsize=500)     # 队列，用于放要爬取的url，长度为500
        self.dbCfg = dbCfg
        self.sem = asyncio.Semaphore(maxCoroNum)     # 使用信号量限制并发的协程的最大数量
        self.tcpConnNum = tcpConnNum    # tcp连接池中最大并发连接数
        self.session = None             # 存放会话连接池的session对象
        self.dbPool = None              # Mysql连接池
        self.listUrlInfo = listUrlInfo
        self.pageRows = pageRows        # 每页列表页有多少篇文章
        self.crawledUrl = set()         # 用于去重的url集合
        self.loop = None                # 事件循环对象
        self.lockForSql = asyncio.Lock()    # 用于同步多个数据入库的协程的锁
        self.monitorCount = 0       # 任务监控计数器
        self.maxMonitorCount = maxMonitorCount

    # 开启事件循环(启动协程)
    def startLoop(self):
        self.loop = asyncio.get_event_loop()
        asyncio.ensure_future(self.start())         # 这里用create_task()会报错,但是ensure_future()不会,两个方法都可以将协程放到事件循环中运行.
        self.loop.run_forever()                     # 必须使用run_forever()不能用run_until_complete()
        # self.loop.run_until_complete(self.start())          # 将主协程放入事件循环的任务列表中开始运行主携程，这行代码会等待
        # self.loop.run_until_complete(asyncio.sleep(0.25))   # 所有协程结束运行后，睡个0.25秒，让所有tcp连接，mysql连接都断开后才结束整个线程以免报警告

    # 主协程
    async def start(self):
        # 在开始爬取之前，先创建mysql连接池
        self.pool = await aiomysql.create_pool(loop=self.loop, **self.dbCfg)


        # 开启 produceListUrl 和 consume 这两个协程，将他们加入到任务列表中开始执行
        asyncio.create_task(self.produceListUrl())
        asyncio.create_task(self.consume())

        # 开启一个monitor协程用于所有任务完成的时候停止事件循环
        asyncio.create_task(self.monitor())

    # 用于监控任务队列 self.queue 是否长时间没有任务, 如果检查到self.queue为空的次数超过self.maxMonitorCount规定的次数则停止事件循环,结束整个线程
    async def monitor(self):
        while True:
            if self.queue.qsize() == 0:
                self.monitorCount += 1
            else:
                self.monitorCount = 0

            if self.monitorCount >= self.maxMonitorCount and self.flag:
                print("所有任务已完成, 事件循环停止")
                self.pool.close()           # 先关闭mysql连接池
                await self.session.close()           # 先关闭tcp连接池
                self.loop.stop()            # 然后才关闭事件循环

            # 每0.5秒检测一次任务队列
            await asyncio.sleep(0.5)


    # 生成列表页url（是api接口）
    async def produceListUrl(self):
        for listName, listInfo in self.listUrlInfo.items():
            for page in range(int(listInfo['page'])):
                listUrl = self.baseUrl + "artlist.php?tid=" + str(listInfo['tid']) + "&start=" + str(int(page) * self.pageRows)

                # 将listUrl放到要爬取的url队列中,这个过程可能发生等待
                await self.queue.put({ "url": listUrl, "tname":listName})

    # 消费者，用于从url队列中取出url进行爬取
    async def consume(self):
        # 在开始爬取之前，先创建tcp连接池
        connector = aiohttp.TCPConnector(ssl=False, limit=self.tcpConnNum)
        async with aiohttp.ClientSession(connector=connector) as self.session:
            while True:     # 不停的从self.queue中取出task任务,task任务是我自己封装的一个字典,包含要爬取的url和其他属性
                task = await self.queue.get()  # 从自定义的asyncio队列中取出任务，这个操作可能发生等待（当队列中没有任务时，get方法会等待）

                print("取出任务：" + str(task))
                if task['url'].find('images') > 0:  # 说明该任务是爬取图片
                    coro = self.crawlPicture(task)
                elif task['url'].find('view') > 0:  # 该url是详情页页url
                    coro = self.crawlDetailUrl(task)
                    self.crawledUrl.add(task['url'])  # 将这个详情页url设为已爬取过的url
                else:
                    coro = self.crawlListUrl(task)

                # 上面这几句只是生成一个协程对象而已,下面这句才是将协程放到任务队列中并发运行
                asyncio.create_task(coro)

    # 爬取列表页url,获取其中的详情页url
    async def crawlListUrl(self, task):
        try:
            listUrl, tname = task['url'], task['tname']
            json_data = await self.getUrl(listUrl, type=2)   # 返回的是json字典,里面包含多个详情页url的id

            for data in json_data:
                detailUrl = self.__joinUrl(data['id'], tname)       # 根据详情页id得到详情页url

                if task['url'] in self.crawledUrl:  # 判断详情页url是否已经爬取过
                    continue
                else:
                    await self.queue.put({"url" : detailUrl})     # 将详情页url放入队列待爬取
        except BaseException as e:
            print(task)
            print(e)

    # 爬取详情页url
    async def crawlDetailUrl(self, task):
        html = await self.getUrl(task['url'])

        # 解析html中的内容,返回data字典和imgSrc列表
        data, imgSrcs = self.parseHtml(html, task['url'])     # 纯cpu操作，没有io，无需await

        # 执行数据入库，将数据入库的协程放入任务列表中并发运行
        asyncio.create_task(self.insertDb(data))

        # 将图片放入到url队列中，让负责下载图片的那个协程从队列取出url下载图片(这行代码放在create_task(self.insertDb(data))之后，因为上面那步是无需等待的，下面这句可能会发生等待)
        # (await self.queue.put({ "url": src, "fp":localPath}) for localPath, src in imgSrcs.items())    # 傻呀,用什么生成器表达式呀,生成器表达式定义的时候,里面的代码不会执行的!
        [await self.queue.put({ "url": src, "fp":localPath}) for localPath, src in imgSrcs.items()]

    # 爬取详情页中的图片
    async def crawlPicture(self, task):
        print("爬取图片")
        print(task)
        src, localPath = task['url'], task['fp']
        localPathList = localPath.split('/')
        localPathList.pop()
        dirPath = '/'.join(localPathList)

        if not os.path.exists(dirPath):
            os.makedirs(dirPath)

        # 下载图片
        blob = await self.getUrl(src, type = 3)

        # 将图片异步写入到本地文件
        async with aiofiles.open(localPath, mode='wb') as f:
            await f.write(blob)

    # 数据入库
    async def insertDb(self, data):
        async with self.pool.acquire() as conn:   # 从mysql连接池获取一个连接
            cursor = await conn.cursor()    # 获取光标
            # async with conn.cursor() as cursor:    # 获取光标
            data['id'] = None
            content = data.pop("content")   # 将content字段从字典中取出

            fieldNameStr = '`,`'.join(data.keys())    # 拼接字段名
            fieldNameStr = '(`' + fieldNameStr + "`)"
            fieldValueStr = '%s,' * len(data)
            fieldValueStr = "(" + fieldValueStr.strip(",") + ")"
            sqlArt = "insert ignore into article " + fieldNameStr + " values " + fieldValueStr
            sqlArtContent = 'insert into art_content (`aid`, `content`) values (%s, %s)'

            # 数据插入文章表 和 文章详情表，由于需要获取插入到文章表的id作为下次插入文章详情表的aid，所以需要把这两条插入语句作为1个原子操作
            # 可以通过加锁的方式把这两句插入语句的执行作为原子操作
            async with self.lockForSql:
                try:
					self.flag = False
                    await cursor.execute(sqlArt, tuple(data.values()))      #数据插入文章表
                    await cursor.execute(sqlArtContent, (cursor.lastrowid, content))    # 文章内容插入文章详情表
                except BaseException as e:
                    print("数据插入报错")
                    print(e)
                finally:
                    # 关闭cursor
					self.flag = True
                    await cursor.close()

    # 拼接详情页url
    def __joinUrl(self, article_id, cate_name):
        return self.baseUrl.strip("/") + "/" + cate_name + '/' + 'view/' + str(article_id) + ".html"

    # 解析详情页html内容
    def parseHtml(self, html, detailUrl):
        soup = BeautifulSoup(html, "html.parser")
        data = {}

        data['title'] = soup.find('h1').get_text()
        artInfo = soup.find('div', class_="artinfo").find_all("span")  # 文章的发布时间，观看数和来源
        data['time'] = self.__strtotime(artInfo[0].get_text())
        data['view'] = int(artInfo[1].get_text().strip(''))
        data['source'] = artInfo[2].get_text().replace("来源：", '') if len(artInfo) > 2 else ''
        data['url'] = detailUrl
        data['tid'] = str([val['tid'] for key, val in self.listUrlInfo.items() if key in detailUrl][0])

        # content字段单独处理
        content = str(soup.find('div', class_='article'))
        data['content'], imgSrcs = self.handleContent(content)  # 该方法会将content中的图片地址获取，并将content中的<img>标签替换为图片下载后的本地路径（但是不会执行下载图片）

        return data, imgSrcs

    # 处理文章Content，搜集文章内容中的图片并返回
    def handleContent(self, content):
        # 用正则将content中的img标签都提取出来
        pattern = re.compile('<img.*?src=["\'](.*?)["\'].*?>', re.DOTALL)

        # 将内容标签变为字符串类型
        contentStr = str(content)

        # 正则替换contentStr中的图片src为下载到本地后的图片地址。并将远程图片的Src记录下来放到队列中待爬
        imgSrcs = {}  # 存放远程图片Src用于之后爬取图片
        regRes = pattern.findall(contentStr)

        for src in regRes:
            localPath = self.renamePic(src, self.picPath)
            imgSrcs[localPath] = src
            contentStr = contentStr.replace(src, localPath)
        # contentStr = pattern.sub(lambda x: (imgSrcs.append(x.group(1)), x.group(0).replace(x.group(1), self.renamePic(x.group(1))))[-1],contentStr)

        return contentStr, imgSrcs

    # 替换文章Src
    def renamePic(self, picUrl, dirPath):
        dirPath = dirPath.strip('/') + '/'  # 存放下载图片的目录路径
        m = hashlib.md5()
        m.update(picUrl.encode())
        fn = m.hexdigest()
        fn = fn[8:24] + '.jpg'

        return dirPath + fn

    # 爬取单个url, type为1返回utf-8的编码格式，2是返回json格式，3是返回二进制格式
    async def getUrl(self, url, type = 1):
        async with self.sem:  # 信号量限制getUrl协程的个数，当并发的getUrl协程数量超过 maxCoroNum的时候，此行代码会发生等待而切换协程
            try:
                print("正在爬取 " + url)
                async with self.session.get(url) as resp:  # 请求url，返回resp对象内含响应内容
                    if resp.status in [200, 201]:
                        if type == 1:
                            responseBody = await resp.text()
                        elif type == 2:
                            # 这里有个小坑，resp.json()方法有一个参数content_type默认为'application/json'，如果爬取到的页面的content-type和json方法的content_type参数不同就会报错（详情可以查看resp.json源码）
                            # 而我要爬的列表页的响应头的content-type是"text/html", 所以这里应该将content_type参数设置为None，即不校验 content-type
                            responseBody = await resp.json(content_type=None, encoding='utf-8')
                        else:
                            responseBody = await resp.read()
                        return responseBody
                    else:
                        return None
            except BaseException as e:
                print("发生错误：", e)
                return None

    def __strtotime(self, strTime):
        # 先转换为时间数组
        timeArray = time.strptime(strTime, "%Y-%m-%d %H:%M:%S")

        # 转换为时间戳
        timeStamp = int(time.mktime(timeArray))
        return timeStamp

if __name__ == "__main__":
    dbCfg = {
        "host" : "127.0.0.1",
        "port" : 3306,
        "user" : 'root',
        "password" : 'root',
        "db" : 'test',
        "charset" : "utf8",
        "autocommit" : True     # 自动提交事务
    }

    # 爬取3类列表页，每类列表页爬100页
    listUrlInfo = {
        'bitcoin':{'tid':10, 'page':5},
        'pme':{'tid':9, 'page':5},
        'arts':{'tid':1, 'page':5}
    }

    crawler = Crawler("https://www.fx112.com/", dbCfg, listUrlInfo)
    crawler.startLoop()


注意点：
1.由于有的协程会生产新的url，有的协程会消费（爬取）url，所以这是一个典型的生产者消费者模型，生产者和消费者需要通信，因而会用到队列。我们希望队列的长度是有限制的，所以使用asyncio.Queue队列而不用list。

2.我希望限制做爬取工作的协程的数量，防止爬取过快被封，这里可以使用 asyncio.Semaphore 信号量来限制getUrl这个协程，因为无论是列表页还是详情页的url请求都会在getUrl这个协程内完成，所以限制这个getUrl协程，就可以限制并发请求url的速度。

3.mysql使用mysql连接池复用mysql连接，这样就不用每次操作mysql都连接一次，减少了网络io等待

4.使用tcp连接池复用连接，避免每请求一次要连接一次，节省等待连接的时间

5.由于下载图片会涉及到文件磁盘io，所以将图片内容写入到文件的时候要使用异步文件读写的库aiofiles

6.下面的例子中，跑事件循环的时候要使用run_forever()方法不能用run_until_complete()方法，run_until_complete(self.start())会在主协程的代码运行完毕之后关闭事件循环。如果用 run_until_complete你会发现代码运行不到半秒就结束了，里面的各种爬取url和数据入库的协程都没来得及执行。

7.在数据入库的时候，一篇文章的内容会插入到两个表：文章表article和文章内容表art_content，由于article的id字段和art_content的aid字段是一一对应的，所以要求一篇文章插入到这两个表的insert操作必须是原子性的，这样才能保证先插入article表然后获取到的lastInsertId（最后插入的记录的id）和要插入art_content表的aid是一致的。可以通过添加 asyncio.Lock 互斥锁的方式保证数据一致。一定要加锁（用锁包住这两条insert代码），不然会出现一篇文章的标题是文章A的标题，但文章的内容是文章B的内容。

8.代码中所有出现 async with 的地方都是可能要等待的地方。

9.例子中，有以下几个地方是比较耗io的地方：
爬取url：与服务端建立tcp连接，发送请求(session.get())和接收响应(resp.text()/json()/read())，属于网络io
数据入库：与mysql建立连接，获取光标(conn.cursor())，插入数据(cursor.execute())，属于磁盘io
下载后的图片写入到本地文件：f.write()，属于磁盘io
self.queue队列空了或者满了之后的等待：不属于io操作，但是也会等待。
这些地方都会发生等待，都要加await

像 handleContent,parseHtml和renamePic这样的方法全都是cpu操作，不会发生io等待，所以这些方法不用加async关键字。

10.必须要对爬取速度做出限制，比如 协程 getUrl() 的最大并发个数（会决定并发请求的个数），tcp连接池中并发连接的个数，self.queue队列中任务的最大个数（如果self.queue能容纳的任务数量过多，会导致这个爬取过程是一个倾向于广度优先的爬取，会爬取大量的列表页后才开始爬详情页）。

11.在startLoop中，由于用的是run_forever而不是run_until_complete(),而run_forever()不能传协程参数，所以只能通过asyncio.ensure_future(start())把主协程传入事件循环的任务列表（这里说的任务列表不是self.queue，self.queue是我自己定义的。这里说的任务列表是asyncio内部实现的任务列表）中。
不能使用create_task()而要用ensure_future()，否则会报错。

=========================================================

程序分析：

1. crawler.startLoop()   一开始，startLoop() 会开启事件循环，并开始运行start()这个主协程
此时事件循环中只有一个协程start()在运行。

2.在start()协程中，建立pool连接池，并且将 produceListUrl() 和 consumer() 这两个协程加入到事件循环中。此时start()协程结束(start()协程就直接return了)
此时事件循环中有2个协程produceListUrl() 和 consumer()在并发运行，produceListUrl会不同的往self.queue中生成新任务，直到达到队列的最大任务限制，此时线程会切换到consumer这个协程开始消费self.queue中的任务，而consumer协程内部的子协程还会继续往self.queue中生成task任务。

3.consumer()协程会开一个无限循环，从self.queue中获取task（其实就是url），对每一个task（url）生成一个子协程 coro （这个协程可能是爬详情页，可能是爬列表页，也可能是下载图片），于是从头到尾会生成成千上万个coro协程（不过由于用 Semaphore做了限制，同时存在的coro协程最多只有20个），并且这些coro协程全都是并发的（因为用了 create_task()方法）

对于 crawlListUrl 协程还会继续往self.queue中增加task（crawlListUrl既是消费者也是生产者）

4.crawlDetailUrl协程会继续产生 insertDb()子协程 ,这类子协程负责数据入库，也是和其他协程是并发的（因为用了create_task()方法）

反正凡是会涉及到比较明显的由于io操作的地方，都用create_task()让他们并发执行效率才高。

5.当所有url都已经爬取完毕的时候，除了 consumer() 协程还没结束之外，其他所有的协程都已经结束，而整个线程会被consumer协程的 task = await self.queue.get() 这一句阻塞。
我们可以在运行代码前多写一个协程monitor用于监控 self.queue ，做法很简单，先定义一个self.monitorCount成员变量作为计数器，monitor协程要做的事情就是无限循环，每次循环都查询一次self.queue中的元素个数，如果元素个数为0，则对计数器+1，如果下一次queue中的元素个数不为0，则计数器清0，每次循环都睡0.5秒。如果计数器达到一个我们指定的值比如10，就表示10 * 0.5 = 5秒之内，queue中的任务数一直都是0，就说明所有的爬取任务可能结束，可以在 monitor 协程中关闭事件循环。

也可以不用 monitor 协程，直接 ctrl + c 手动结束

=========================================================
当然，上面的程序还有问题：当我把列表页设置为150，详情页就有1500个，此时会出现爬虫一直在爬，但是一直没有数据入库，直到所有url差不多请求完毕之后才开始数据入库。

这是因为一开始列表页和详情页不停的在self.queue中生成，详情页和列表页又不停的在consume协程中弹出，因而consume生成了太多太多的 crawlDetailUrl 和 crawlListUrl 子协程。create_task()是不用等待的，所以crawlDetailUrl 和 crawlListUrl协程产生的太快太多，而 insertDb 协程内部很多需要等待的地方，所以线程全都切换到consume协程中去制造更多crawlDetailUrl 和 crawlListUrl子协程，而 insertDb 几乎没机会被线程调度。所以才会发生上面的情况。

所以，我将 Semaphore 信号量限制从 getUrl 给挪到了 consume 的死循环中，因为 Semaphore 放在 getUrl 中只是限制了发送请求的速度（限制了请求并发数），而放在 consume 中就可以限制 crawlDetailUrl 和 crawlListUrl协程 的 create_task()到事件循环中的速度（当然，这样也能限制了请求并发数）。这样可以有效降低这两类协程的生成速度。

结果就是会边爬边数据入库。至于self.insertDb是用 create_task 加到事件循环中并发还是用 await 等待执行都可以。

所以我没考虑到的一点是：只想到了要限制请求的并发量，但没想到要限制协程的产生速度，如果爬取的url有几百万个的话，上一个版本的代码运行到后期会变成有几百万个协程存在事件循环的任务列表中，但正在并发向服务器请求的协程可能就几十个，其他几百万协程全都等待 Semaphore 信号量，这和要不要数据入库无关，即使不用数据入库也要限制协程的数量和生成速度。

所以最终的版本如下：
# coding=utf-8

import asyncio, aiohttp, aiomysql, re, hashlib, os, aiofiles, time
from asyncio import Queue
from bs4 import BeautifulSoup

class Crawler:
    def __init__(self, baseUrl, dbCfg, listUrlInfo, picPath = './pic1', pageRows=10, maxCoroNum = 20, tcpConnNum = 20, maxMonitorCount = 10):
        self.baseUrl = baseUrl.strip('/') + "/"
        self.picPath = picPath              # 下载图片到本地的目录
        self.queue = Queue(maxsize=50)     # 队列，用于放要爬取的url，长度为500
        self.dbCfg = dbCfg
        self.sem = asyncio.Semaphore(maxCoroNum)     # 使用信号量限制并发的协程的最大数量
        self.tcpConnNum = tcpConnNum    # tcp连接池中最大并发连接数
        self.session = None             # 存放会话连接池的session对象
        self.dbPool = None              # Mysql连接池
        self.listUrlInfo = listUrlInfo
        self.pageRows = pageRows        # 每页列表页有多少篇文章
        self.crawledUrl = set()         # 用于去重的url集合
        self.loop = None                # 事件循环对象
        self.lockForSql = asyncio.Lock()    # 用于同步多个数据入库的协程的锁
        self.monitorCount = 0       # 任务监控计数器
        self.maxMonitorCount = maxMonitorCount

    # 开启事件循环(启动协程)
    def startLoop(self):
        self.loop = asyncio.get_event_loop()
        asyncio.ensure_future(self.start())         # 这里用create_task()会报错,但是ensure_future()不会,两个方法都可以将协程放到事件循环中运行.
        self.loop.run_forever()                     # 必须使用run_forever()不能用run_until_complete()
        # self.loop.run_until_complete(self.start())          # 将主协程放入事件循环的任务列表中开始运行主携程，这行代码会等待
        # self.loop.run_until_complete(asyncio.sleep(0.25))   # 所有协程结束运行后，睡个0.25秒，让所有tcp连接，mysql连接都断开后才结束整个线程以免报警告

    # 主协程
    async def start(self):
        # 在开始爬取之前，先创建mysql连接池
        self.pool = await aiomysql.create_pool(loop=self.loop, maxsize = 100, minsize = 100, pool_recycle = 100, **self.dbCfg)

        # 开启 produceListUrl 和 consume 这两个协程，将他们加入到任务列表中开始执行
        asyncio.create_task(self.produceListUrl())
        asyncio.create_task(self.consume())

        # 开启一个monitor协程用于所有任务完成的时候停止事件循环
        # asyncio.create_task(self.monitor())

    # 用于监控任务队列 self.queue 是否长时间没有任务, 如果检查到self.queue为空的次数超过self.maxMonitorCount规定的次数则停止事件循环,结束整个线程
    async def monitor(self):
        while True:
            if self.queue.qsize() == 0:
                self.monitorCount += 1
            else:
                self.monitorCount = 0

            if self.monitorCount >= self.maxMonitorCount:
                print("所有任务已完成, 事件循环停止")
                self.pool.close()           # 先关闭mysql连接池
                await self.session.close()           # 先关闭tcp连接池
                self.loop.stop()            # 然后才关闭事件循环

            # 每0.5秒检测一次任务队列
            await asyncio.sleep(0.5)


    # 生成列表页url（是api接口）
    async def produceListUrl(self):
        for listName, listInfo in self.listUrlInfo.items():
            for page in range(int(listInfo['page'])):
                listUrl = self.baseUrl + "artlist.php?tid=" + str(listInfo['tid']) + "&start=" + str(int(page) * self.pageRows)

                # 将listUrl放到要爬取的url队列中,这个过程可能发生等待
                await self.queue.put({ "url": listUrl, "tname":listName})

    # 消费者，用于从url队列中取出url进行爬取
    async def consume(self):
        # 在开始爬取之前，先创建tcp连接池
        connector = aiohttp.TCPConnector(ssl=False, limit=self.tcpConnNum)
        async with aiohttp.ClientSession(connector=connector) as self.session:
            while True:     # 不停的从self.queue中取出task任务,task任务是我自己封装的一个字典,包含要爬取的url和其他属性
                task = await self.queue.get()  # 从自定义的asyncio队列中取出任务，这个操作可能发生等待（当队列中没有任务时，get方法会等待）

                self.sem.acquire()  # 信号量限制并发协程的个数，当并发的coro协程数量超过 maxCoroNum的时候，此行代码会发生等待而切换协程
                print("取出任务：" + str(task))
                if task['url'].find('images') > 0 or task['url'].find('img') > 0:  # 说明该任务是爬取图片
                    coro = self.crawlPicture(task)
                elif task['url'].find('view') > 0:  # 该url是详情页页url
                    coro = self.crawlDetailUrl(task)
                    self.crawledUrl.add(task['url'])  # 将这个详情页url设为已爬取过的url
                else:
                    coro = self.crawlListUrl(task)

                # 上面这几句只是生成一个协程对象而已,下面这句才是将协程放到任务队列中并发运行
                asyncio.create_task(coro)

    # 爬取单个url, type为1返回utf-8的编码格式，2是返回json格式，3是返回二进制格式
    async def getUrl(self, url, type=1):
        # async with self.sem:  # 信号量限制getUrl协程的个数，当并发的getUrl协程数量超过 maxCoroNum的时候，此行代码会发生等待而切换协程
        try:
            async with self.session.get(url) as resp:  # 请求url，返回resp对象内含响应内容
                if resp.status in [200, 201]:
                    if type == 1:
                        responseBody = await resp.text()
                    elif type == 2:
                        # 这里有个小坑，resp.json()方法有一个参数content_type默认为'application/json'，如果爬取到的页面的content-type和json方法的content_type参数不同就会报错（详情可以查看resp.json源码）
                        # 而我要爬的列表页的响应头的content-type是"text/html", 所以这里应该将content_type参数设置为None，即不校验 content-type
                        responseBody = await resp.json(content_type=None, encoding='utf-8')
                    else:
                        responseBody = await resp.read()
                    print("成功爬取 " + url)
                    return responseBody
                else:
                    print("响应码错误:%s | %s" % (str(resp.status), resp.url))
                    return None
        except BaseException as e:
            print("发生错误：", e)
            return None

    # 爬取列表页url,获取其中的详情页url
    async def crawlListUrl(self, task):
        # try:
        listUrl, tname = task['url'], task['tname']
        json_data = await self.getUrl(listUrl, type=2)   # 返回的是json字典,里面包含多个详情页url的id

        for data in json_data:
            detailUrl = self.__joinUrl(data['id'], tname)       # 根据详情页id得到详情页url

            if task['url'] in self.crawledUrl:  # 判断详情页url是否已经爬取过
                continue
            else:
                await self.queue.put({"url" : detailUrl})     # 将详情页url放入队列待爬取

        self.sem.release()
        # except BaseException as e:
        #     print(task)
        #     print("出现错误: " , e)

    # 爬取详情页url
    async def crawlDetailUrl(self, task):
        try:
            html = await self.getUrl(task['url'])

            # 解析html中的内容,返回data字典和imgSrc列表
            data, imgSrcs = self.parseHtml(html, task['url'])     # 纯cpu操作，没有io，无需await

            # 执行数据入库，将数据入库的协程放入任务列表中并发运行, 下面我用了2种写法, 两种都行,后者会边爬边入库,前者会一直爬爬了很多很多url后才开始入库
            asyncio.create_task(self.insertDb(data))  # 把 insertDb 放到事件循环并发执行,create_task方法不会发生任何等待
            # await self.insertDb(data)                     # 让crawlDetailUrl协程 等待 insertDb 子协程运行完

            # 将图片放入到url队列中，让负责下载图片的那个协程从队列取出url下载图片
            # (await self.queue.put({ "url": src, "fp":localPath}) for localPath, src in imgSrcs.items())    # 傻呀,用什么生成器表达式呀,生成器表达式定义的时候,里面的代码不会执行的!
            [await self.queue.put({ "url": src, "fp":localPath}) for localPath, src in imgSrcs.items()]
        except BaseException as e:
            print("爬取详情页发生错误： ", e)
        finally:
            self.sem.release()

    # 爬取详情页中的图片
    async def crawlPicture(self, task):
        print("爬取图片")
        print(task)
        src, localPath = task['url'], task['fp']
        localPathList = localPath.split('/')
        localPathList.pop()
        dirPath = '/'.join(localPathList)

        if not os.path.exists(dirPath):
            os.makedirs(dirPath)

        # 下载图片
        blob = await self.getUrl(src, type = 3)

        # 将图片异步写入到本地文件
        try:
            async with aiofiles.open(localPath, mode='wb') as f:
                await f.write(blob)
        except BaseException as e:
            print("图片写入本地错误:", e)
        finally:
            self.sem.release()
            f.close()
    # 数据入库
    async def insertDb(self, data):
        try:
            async with self.pool.acquire() as conn:   # 从mysql连接池获取一个连接
                cursor = await conn.cursor()    # 获取光标
                # async with conn.cursor() as cursor:    # 获取光标
                data['id'] = None
                content = data.pop("content")   # 将content字段从字典中取出

                fieldNameStr = '`,`'.join(data.keys())    # 拼接字段名
                fieldNameStr = '(`' + fieldNameStr + "`)"
                fieldValueStr = '%s,' * len(data)
                fieldValueStr = "(" + fieldValueStr.strip(",") + ")"
                sqlArt = "insert ignore into article " + fieldNameStr + " values " + fieldValueStr
                sqlArtContent = 'insert into art_content (`aid`, `content`) values (%s, %s)'

                # 数据插入文章表 和 文章详情表，由于需要获取插入到文章表的id作为下次插入文章详情表的aid，所以需要把这两条插入语句作为1个原子操作
                # 可以通过加锁的方式把这两句插入语句的执行作为原子操作
                async with self.lockForSql:
                    try:
                        await cursor.execute(sqlArt, tuple(data.values()))      #数据插入文章表
                        await cursor.execute(sqlArtContent, (cursor.lastrowid, content))    # 文章内容插入文章详情表
                        print("数据入库成功: %s" % data['url'])
                    except BaseException as e:
                        print("数据插入报错")
                        print(e)
                    finally:
                        # 关闭cursor
                        await cursor.close()
        except BaseException as e:
            print("mysql 连接错误:", e)
            print("mysql连接错误对应url %s" % data['url'])

    # 拼接详情页url
    def __joinUrl(self, article_id, cate_name):
        return self.baseUrl.strip("/") + "/" + cate_name + '/' + 'view/' + str(article_id) + ".html"

    # 解析详情页html内容
    def parseHtml(self, html, detailUrl):
        soup = BeautifulSoup(html, "html.parser")
        data = {}

        data['title'] = soup.find('h1').get_text()
        artInfo = soup.find('div', class_="artinfo").find_all("span")  # 文章的发布时间，观看数和来源
        data['time'] = self.__strtotime(artInfo[0].get_text())
        data['view'] = int(artInfo[1].get_text().strip(''))
        data['source'] = artInfo[2].get_text().replace("来源：", '') if len(artInfo) > 2 else ''
        data['url'] = detailUrl
        data['tid'] = str([val['tid'] for key, val in self.listUrlInfo.items() if key in detailUrl][0])

        # content字段单独处理
        content = str(soup.find('div', class_='article'))
        data['content'], imgSrcs = self.handleContent(content)  # 该方法会将content中的图片地址获取，并将content中的<img>标签替换为图片下载后的本地路径（但是不会执行下载图片）

        return data, imgSrcs

    # 处理文章Content，搜集文章内容中的图片并返回
    def handleContent(self, content):
        # 用正则将content中的img标签都提取出来
        pattern = re.compile('<img.*?src=["\'](.*?)["\'].*?>', re.DOTALL)

        # 将内容标签变为字符串类型
        contentStr = str(content)

        # 正则替换contentStr中的图片src为下载到本地后的图片地址。并将远程图片的Src记录下来放到队列中待爬
        imgSrcs = {}  # 存放远程图片Src用于之后爬取图片
        regRes = pattern.findall(contentStr)

        for src in regRes:
            localPath = self.renamePic(src, self.picPath)
            imgSrcs[localPath] = src
            contentStr = contentStr.replace(src, localPath)
        # contentStr = pattern.sub(lambda x: (imgSrcs.append(x.group(1)), x.group(0).replace(x.group(1), self.renamePic(x.group(1))))[-1],contentStr)

        return contentStr, imgSrcs

    # 替换文章Src
    def renamePic(self, picUrl, dirPath):
        dirPath = dirPath.strip('/') + '/'  # 存放下载图片的目录路径
        m = hashlib.md5()
        m.update(picUrl.encode())
        fn = m.hexdigest()
        fn = fn[8:24] + '.jpg'

        return dirPath + fn

    def __strtotime(self, strTime):
        # 先转换为时间数组
        timeArray = time.strptime(strTime, "%Y-%m-%d %H:%M:%S")

        # 转换为时间戳
        timeStamp = int(time.mktime(timeArray))
        return timeStamp

if __name__ == "__main__":
    dbCfg = {
        "host" : "127.0.0.1",
        "port" : 3306,
        "user" : 'root',
        "password" : 'root',
        "db" : 'test',
        "charset" : "utf8",
        "autocommit" : True     # 自动提交事务
    }

    # 爬取3类列表页，每类列表页爬100页
    listUrlInfo = {
        'bitcoin':{'tid':10, 'page':300},
        'pme':{'tid':9, 'page':300},
        'arts':{'tid':1, 'page':300}
    }

    crawler = Crawler("https://www.fx112.com/", dbCfg, listUrlInfo, maxCoroNum = 100, tcpConnNum = 40, maxMonitorCount = 100)
    crawler.startLoop()


注意： Semaphore 的释放要在 crawlDetailUrl,crawlListUrl和 crawlPicture 方法内部，不要放在consume中，否则semaphore相当于acquire()了之后又立刻release()了。
=========================================================================

上面这个版本在一开始虽然也会数据入库，但是入库的并发远低于爬取的并发。所以如果想要让入库的速度跟得上爬取的速度，可以这样做：再创建一个队列 DbQueue 用于放入库的任务，将 insertDb 放到 start 主协程中和consume协程同级，insertDb从DbQueue 中取出入库的任务执行入库操作，这样 insertDb 的并发速度就完全跟得上爬取的速度。

# coding=utf-8

import asyncio, aiohttp, aiomysql, re, hashlib, os, aiofiles, time
from asyncio import Queue
from bs4 import BeautifulSoup

class Crawler:
    def __init__(self, baseUrl, dbCfg, listUrlInfo, picPath = './pic2', pageRows=10, maxCoroNum = 20, tcpConnNum = 20, maxMonitorCount = 10, insertRows = 20):
        self.baseUrl = baseUrl.strip('/') + "/"
        self.picPath = picPath              # 下载图片到本地的目录
        self.queue = Queue(maxsize=50)     # 队列，用于放要爬取的url，长度为50
        self.dbCfg = dbCfg
        self.sem = asyncio.Semaphore(maxCoroNum)     # 使用信号量限制并发的coro协程的最大数量
        self.tcpConnNum = tcpConnNum    # tcp连接池中最大并发连接数
        self.session = None             # 存放会话连接池的session对象
        self.dbPool = None              # Mysql连接池
        self.listUrlInfo = listUrlInfo
        self.pageRows = pageRows        # 每页列表页有多少篇文章
        self.crawledUrl = set()         # 用于去重的url集合
        self.loop = None                # 事件循环对象
        self.lockForSql = asyncio.Lock()    # 用于同步多个数据入库的协程的锁
        self.monitorCount = 0       # 任务监控计数器
        self.maxMonitorCount = maxMonitorCount

        ############################################
        self.dbQueue = Queue(maxsize=500)       # 存放数据入库任务
        self.dbSem = asyncio.Semaphore(maxCoroNum)      # 用于限制并发的 insertDb 协程的最大数量

    # 开启事件循环(启动协程)
    def startLoop(self):
        self.loop = asyncio.get_event_loop()
        asyncio.ensure_future(self.start())         # 这里用create_task()会报错,但是ensure_future()不会,两个方法都可以将协程放到事件循环中运行.
        self.loop.run_forever()                     # 必须使用run_forever()不能用run_until_complete()
        # self.loop.run_until_complete(self.start())          # 将主协程放入事件循环的任务列表中开始运行主携程，这行代码会等待
        # self.loop.run_until_complete(asyncio.sleep(0.25))   # 所有协程结束运行后，睡个0.25秒，让所有tcp连接，mysql连接都断开后才结束整个线程以免报警告

    # 主协程
    async def start(self):
        # 在开始爬取之前，先创建mysql连接池
        self.pool = await aiomysql.create_pool(loop=self.loop, maxsize = 50, minsize = 50, **self.dbCfg)

        # 开启 produceListUrl 和 consume 这两个协程，将他们加入到任务列表中开始执行
        asyncio.create_task(self.produceListUrl())
        asyncio.create_task(self.consume())
        asyncio.create_task(self.dbConsume())       # 开启 dbConsume 协程不断生成 insertDb子协程进行数据入库

        # 开启一个monitor协程用于所有任务完成的时候停止事件循环
        # asyncio.create_task(self.monitor())

    # 用于监控任务队列 self.queue 是否长时间没有任务, 如果检查到self.queue为空的次数超过self.maxMonitorCount规定的次数则停止事件循环,结束整个线程
    async def monitor(self):
        while True:
            if self.queue.qsize() == 0:
                self.monitorCount += 1
            else:
                self.monitorCount = 0

            if self.monitorCount >= self.maxMonitorCount:
                print("所有任务已完成, 事件循环停止")
                self.pool.close()           # 先关闭mysql连接池
                await self.session.close()           # 先关闭tcp连接池
                self.loop.stop()            # 然后才关闭事件循环

            # 每0.5秒检测一次任务队列
            await asyncio.sleep(0.5)


    # 生成列表页url（是api接口）
    async def produceListUrl(self):
        for listName, listInfo in self.listUrlInfo.items():
            for page in range(int(listInfo['page'])):
                listUrl = self.baseUrl + "artlist.php?tid=" + str(listInfo['tid']) + "&start=" + str(int(page) * self.pageRows)

                # 将listUrl放到要爬取的url队列中,这个过程可能发生等待
                await self.queue.put({ "url": listUrl, "tname":listName})

    # 消费者，用于从url队列中取出url进行爬取
    async def consume(self):
        # 在开始爬取之前，先创建tcp连接池
        connector = aiohttp.TCPConnector(ssl=False, limit=self.tcpConnNum)
        async with aiohttp.ClientSession(connector=connector) as self.session:
            while True:     # 不停的从self.queue中取出task任务,task任务是我自己封装的一个字典,包含要爬取的url和其他属性
                task = await self.queue.get()  # 从自定义的asyncio队列中取出任务，这个操作可能发生等待（当队列中没有任务时，get方法会等待）

                self.sem.acquire()
                print("取出任务：" + str(task))
                if task['url'].find('images') > 0 or task['url'].find('img') > 0:  # 说明该任务是爬取图片
                    coro = self.crawlPicture(task)
                elif task['url'].find('view') > 0:  # 该url是详情页页url
                    coro = self.crawlDetailUrl(task)
                    self.crawledUrl.add(task['url'])  # 将这个详情页url设为已爬取过的url
                else:
                    coro = self.crawlListUrl(task)

                # 上面这几句只是生成一个协程对象而已,下面这句才是将协程放到任务队列中并发运行
                asyncio.create_task(coro)

    # 爬取单个url, type为1返回utf-8的编码格式，2是返回json格式，3是返回二进制格式
    async def getUrl(self, url, type=1):
        # async with self.sem:  # 信号量限制getUrl协程的个数，当并发的getUrl协程数量超过 maxCoroNum的时候，此行代码会发生等待而切换协程
        try:
            async with self.session.get(url) as resp:  # 请求url，返回resp对象内含响应内容
                if resp.status in [200, 201]:
                    if type == 1:
                        responseBody = await resp.text()
                    elif type == 2:
                        # 这里有个小坑，resp.json()方法有一个参数content_type默认为'application/json'，如果爬取到的页面的content-type和json方法的content_type参数不同就会报错（详情可以查看resp.json源码）
                        # 而我要爬的列表页的响应头的content-type是"text/html", 所以这里应该将content_type参数设置为None，即不校验 content-type
                        responseBody = await resp.json(content_type=None, encoding='utf-8')
                    else:
                        responseBody = await resp.read()
                    print("成功爬取 " + url)
                    return responseBody
                else:
                    print("响应码错误:%s | %s" % (str(resp.status), resp.url))
                    return None
        except BaseException as e:
            print("连接发生错误：", e)
            return None

    # 爬取列表页url,获取其中的详情页url
    async def crawlListUrl(self, task):
        try:
            listUrl, tname = task['url'], task['tname']
            json_data = await self.getUrl(listUrl, type=2)   # 返回的是json字典,里面包含多个详情页url的id

            for data in json_data:
                detailUrl = self.__joinUrl(data['id'], tname)       # 根据详情页id得到详情页url

                if task['url'] in self.crawledUrl:  # 判断详情页url是否已经爬取过
                    continue
                else:
                    await self.queue.put({"url" : detailUrl})     # 将详情页url放入队列待爬取
        except BaseException as e:
            print(task)
            print("出现错误: " , e)
        finally:
            self.sem.release()

    # 爬取详情页url
    async def crawlDetailUrl(self, task):
        try:
            html = await self.getUrl(task['url'])

            # 解析html中的内容,返回data字典和imgSrc列表
            data, imgSrcs = self.parseHtml(html, task['url'])     # 纯cpu操作，没有io，无需await

            await self.dbQueue.put(data)  # 添加 数据入库 任务

            # 将图片放入到url队列中，让负责下载图片的那个协程从队列取出url下载图片
            # (await self.queue.put({ "url": src, "fp":localPath}) for localPath, src in imgSrcs.items())    # 傻呀,用什么生成器表达式呀,生成器表达式定义的时候,里面的代码不会执行的!
            [await self.queue.put({ "url": src, "fp":localPath}) for localPath, src in imgSrcs.items()]
        except BaseException as e:
            print("爬取详情页发生错误： ", e)
        finally:
            self.sem.release()

    # 爬取详情页中的图片
    async def crawlPicture(self, task):
        print("爬取图片")
        print(task)
        src, localPath = task['url'], task['fp']
        localPathList = localPath.split('/')
        localPathList.pop()
        dirPath = '/'.join(localPathList)

        if not os.path.exists(dirPath):
            os.makedirs(dirPath)

        # 下载图片
        blob = await self.getUrl(src, type = 3)

        # 将图片异步写入到本地文件
        try:
            async with aiofiles.open(localPath, mode='wb') as f:
                await f.write(blob)
        except BaseException as e:
            print("图片写入本地错误:", e)
        finally:
            self.sem.release()
            f.close()

    # 数据入库任务的消费者
    async def dbConsume(self):
        while True:     # 循环取出dbQueue任务
            dbTask = await self.dbQueue.get()

            self.dbSem.acquire()
            asyncio.create_task(self.insertDb(dbTask))


    # 数据入库
    async def insertDb(self, data):
        try:
            async with self.pool.acquire() as conn:   # 从mysql连接池获取一个连接
                cursor = await conn.cursor()    # 获取光标
                # async with conn.cursor() as cursor:    # 获取光标
                data['id'] = None
                content = data.pop("content")   # 将content字段从字典中取出

                fieldNameStr = '`,`'.join(data.keys())    # 拼接字段名
                fieldNameStr = '(`' + fieldNameStr + "`)"
                fieldValueStr = '%s,' * len(data)
                fieldValueStr = "(" + fieldValueStr.strip(",") + ")"
                sqlArt = "insert ignore into article " + fieldNameStr + " values " + fieldValueStr
                sqlArtContent = 'insert into art_content (`aid`, `content`) values (%s, %s)'

                # 数据插入文章表 和 文章详情表，由于需要获取插入到文章表的id作为下次插入文章详情表的aid，所以需要把这两条插入语句作为1个原子操作
                # 可以通过加锁的方式把这两句插入语句的执行作为原子操作
                async with self.lockForSql:
                    try:
                        await cursor.execute(sqlArt, tuple(data.values()))      #数据插入文章表
                        await cursor.execute(sqlArtContent, (cursor.lastrowid, content))    # 文章内容插入文章详情表
                        print("数据入库成功: %s" % data['url'])
                    except BaseException as e:
                        print("数据插入报错")
                        print(e)
                    finally:
                        # 关闭cursor
                        await cursor.close()
        except BaseException as e:
            print("mysql 连接错误:", e)
            print("mysql连接错误对应url %s" % data['url'])
        finally:
            self.dbSem.release()

    # 拼接详情页url
    def __joinUrl(self, article_id, cate_name):
        return self.baseUrl.strip("/") + "/" + cate_name + '/' + 'view/' + str(article_id) + ".html"

    # 解析详情页html内容
    def parseHtml(self, html, detailUrl):
        soup = BeautifulSoup(html, "html.parser")
        data = {}

        data['title'] = soup.find('h1').get_text()
        artInfo = soup.find('div', class_="artinfo").find_all("span")  # 文章的发布时间，观看数和来源
        data['time'] = self.__strtotime(artInfo[0].get_text())
        data['view'] = int(artInfo[1].get_text().strip(''))
        data['source'] = artInfo[2].get_text().replace("来源：", '') if len(artInfo) > 2 else ''
        data['url'] = detailUrl
        data['tid'] = str([val['tid'] for key, val in self.listUrlInfo.items() if key in detailUrl][0])

        # content字段单独处理
        content = str(soup.find('div', class_='article'))
        data['content'], imgSrcs = self.handleContent(content)  # 该方法会将content中的图片地址获取，并将content中的<img>标签替换为图片下载后的本地路径（但是不会执行下载图片）

        return data, imgSrcs

    # 处理文章Content，搜集文章内容中的图片并返回
    def handleContent(self, content):
        # 用正则将content中的img标签都提取出来
        pattern = re.compile('<img.*?src=["\'](.*?)["\'].*?>', re.DOTALL)

        # 将内容标签变为字符串类型
        contentStr = str(content)

        # 正则替换contentStr中的图片src为下载到本地后的图片地址。并将远程图片的Src记录下来放到队列中待爬
        imgSrcs = {}  # 存放远程图片Src用于之后爬取图片
        regRes = pattern.findall(contentStr)

        for src in regRes:
            localPath = self.renamePic(src, self.picPath)
            imgSrcs[localPath] = src
            contentStr = contentStr.replace(src, localPath)
        # contentStr = pattern.sub(lambda x: (imgSrcs.append(x.group(1)), x.group(0).replace(x.group(1), self.renamePic(x.group(1))))[-1],contentStr)

        return contentStr, imgSrcs

    # 替换文章Src
    def renamePic(self, picUrl, dirPath):
        dirPath = dirPath.strip('/') + '/'  # 存放下载图片的目录路径
        m = hashlib.md5()
        m.update(picUrl.encode())
        fn = m.hexdigest()
        fn = fn[8:24] + '.jpg'

        return dirPath + fn

    def __strtotime(self, strTime):
        # 先转换为时间数组
        timeArray = time.strptime(strTime, "%Y-%m-%d %H:%M:%S")

        # 转换为时间戳
        timeStamp = int(time.mktime(timeArray))
        return timeStamp

if __name__ == "__main__":
    dbCfg = {
        "host" : "127.0.0.1",
        "port" : 3306,
        "user" : 'root',
        "password" : 'root',
        "db" : 'test',
        "charset" : "utf8",
        "autocommit" : True     # 自动提交事务
    }

    # 爬取3类列表页，每类列表页爬100页
    listUrlInfo = {
        'bitcoin':{'tid':10, 'page':300},
        'pme':{'tid':9, 'page':300},
        'arts':{'tid':1, 'page':300}
    }

    crawler = Crawler("https://www.fx112.com/", dbCfg, listUrlInfo, maxCoroNum = 50, tcpConnNum = 40, maxMonitorCount = 100)
    crawler.startLoop()



这里有一篇关于协程原理的文章可以看看：
https://www.zhihu.com/question/294188439/answer/555273313