以下内容全部来自官方文档，经过翻译和小小的修饰后贴出来的，本文仅介绍aiohttp库作为客户端的用法
文档地址：https://docs.aiohttp.org/en/stable

1.请求一个url

import aiohttp, asyncio

async def fetch(url):
	async with aiohttp.request('GET', url) as resp:
		if resp.status == 200:
			print(await resp.text())
			
loop = asyncio.get_event_loop()
loop.run_until_complete(fetch("http://zbpblog.com"))

进入async with 代码块时，会建立连接，发起get请求，这个过程是要等待的，线程会切换到其他协程运行。当服务器返回响应时，客户端读事件就绪，协程恢复运行，将响应封装为 resp 对象。
resp.text()	 # 将响应内容解码为utf-8 ，这个过程也是要等待的，所以要用到await。
使用这种方式的请求url所建立的连接一般都是短连接，请求完一个url后，服务端会自动关闭连接，下次请求的时候，客户端要发起新的连接请求，也就是要重新建立连接。
这对于爬取多个页面的程序来说效率是比较低的。此时可以使用连接池请求。



2.使用连接池请求
下面直接使用官网的例子：
async with aiohttp.ClientSession() as session:
    async with session.get('http://httpbin.org/get') as resp:
        print(resp.status)
        print(await resp.text())
		
调用 ClientSession 时，会建立一个 session 会话对象，这个对象中包含了一个连接池，里面放置了多个连接，而且里面的连接都是keep-alive长连接，可以用来发起多次请求。

在上面的例子中，
async with aiohttp.ClientSession() as session 会创建一个连接池

而 async with session.get('http://httpbin.org/get') as resp 会从连接池中取出一个连接发起get请求

当跳出 async with session.get(xxx) 这个代码块的时候，会将连接放回连接池以备复用。

当跳出 async with ClientSession() as session 这个代码块的时候，会隐式调用 session.close() 方法将连接池中所有的连接关闭。

注意：上例的 session 对象不是一个连接，而是一个连接池，里面包含多个连接。


按照官方的说法，不能没请求一次都建立一个连接池，而是建立一个连接池，用里面的连接完成所有请求（这意味着我要在一个async with ClientSession()代码块中完成所有的请求）。除非你要爬取多个不同域名的站点，此时就要建立多个连接池。

如果不使用 async with 结构建立连接池的话，可以用下面这种方式：
session = aiohttp.ClientSession()
async with session.get('...'):
    # ...
await session.close()

二者是等效的。


下面介绍以下 ClientSession() 的参数：
用的比较多的是connector,headers,cookies。headers和cookies写过爬虫的可能都认识了，这里只谈一下connector。

connector是aiohttp客户端API的传输工具。并发量控制，ssl证书验证，都可通过connector设置，然后传入ClientSession。

标准connector有两种：
A. TCPConnector用于常规TCP套接字（同时支持HTTP和 HTTPS方案）(绝大部分情况使用这种)。

B. UnixConnector 用于通过UNIX套接字进行连接（主要用于测试）。

所有连接器类都应继承自BaseConnector。

例子：
#创建一个TCPConnector
conn=aiohttp.TCPConnector(verify_ssl=False)
#作为参数传入ClientSession
async with aiohttp.ClientSession(connector=conn) as session:
	# ...
	pass
	

TCPConnector比较重要的参数有

verify_ssl（bool）C布尔值，对HTTPS请求执行SSL证书验证 （默认情况下启用）。当要跳过对具有无效证书的站点的验证时可设置为False。

limit（int）C整型，连接池中的并发连接数。如果为limit为 None则connector没有限制（默认值：100）。

limit_per_host（int）C限制同时连接到同一端点的总数。如果(host, port, is_ssl)三者相同，则端点是相同的。如果为limit=0，则connector没有限制（默认值：0）。


在官方文档中，有对 ClientSession 连接池关闭的说明如下：
在离开async with 代码块或者直接调用 session.close()方法之后，连接没有马上关闭，而是会在一小个瞬间之后才关闭。但是如果事件循环在连接关闭之前就先停止了的话就会报一个警告：ResourceWarning: unclosed transport。

为了避免这种情况，要在关闭事件循环之前设置一个小小的延迟，让连接先自动关闭了先，再退出事件循环。

对于没有设置SSL的 ClientSession ，可以做一个简单的 await asyncio.sleep(0) 延迟即可。

对于设置了SSL的 ClientSession，需要做一个0.25秒的延迟

下面是官方例子：
async def read_website():
    async with aiohttp.ClientSession() as session:
        async with session.get('http://example.org/') as resp:
            await resp.read()

loop = asyncio.get_event_loop()
loop.run_until_complete(read_website())
loop.run_until_complete(asyncio.sleep(0.25))	# 这里相当于是在 read_website() 协程执行完了之后，又往事件循环中添加了一个睡0.25秒的协程，这个协程和 read_website 协程是串行的。所以执行到这行的时候，线程会切换到这个协程睡0.25秒，此时整个线程是阻塞的。
loop.close()		# 关闭事件循环




3. 请求时传入请求参数/header/cookie 
假设是通过连接池的方式进行连接，则header和cookie的传递可以在 ClientSession(headers=headers, cookies=cookies) 建立连接池的时候就传进去, headers和cookies都是一个字典。

如果是请求参数的传递，可以这样：
session.get(url, params=params)		# params 是一个字典
session.post(url, params=params,data=data)	# params是get参数，data是post参数也是字典

还可以传递 ssl = False 参数，不验证ssl证书

如果想发起一个json请求，可以传递json格式的数据，可以这样
session.post(url, json=json_data)		# json_data 也是一个字典

当然也可以在 get 和 post 方法上传递 cookies 和 headers 参数

这里还介绍了如何上传一个文件：
url = 'http://httpbin.org/post'
files = {'file': open('report.xls', 'rb')}

await session.post(url, data=files)

或者

url = 'http://httpbin.org/post'
data = FormData()
data.add_field('file',
               open('report.xls', 'rb'),
               filename='report.xls',
               content_type='application/vnd.ms-excel')

await session.post(url, data=data)


4.resp 对象
resp对象将请求返回的结果给封装了起来。

resp的方法和属性：
resp.url 		# 请求的url
resp.status 	# 状态码
await resp.text()    # 解编码后的响应内容，返回的是一个列表
await resp.read()  # 未解编码的响应内容，是二进制的格式，像下载图片就可以用这个
await resp.json()  # 如果你请求的是一个返回json格式的接口，可以用json()方法将响应解析为json格式
resp.cookies		# 响应返回的cookie 是一个字典
resp.headers


5.大文件下载
通过上面的text(),read()和json()方法获取响应内容的时候，会直接把响应内容写入到用户态的内存中，所以加入你想下载几个G的内容比如电影视频就不适合用上面的方式获取响应内容，否则内存会被撑爆的。
此时可以使用 resp.content 来获取响应内容，他会以文件流的形式把响应一点一点的返回给用户态程序。

官方例子：
with open(filename, 'wb') as fd:
    while True:		# 每次读取1024个字节并写入到本地文件中。
        chunk = await resp.content.read(1024)
        if not chunk:
            break
        fd.write(chunk)
		
官方手册给的例子是用的python内置的文件io方法，这是同步阻塞的方法。但是在协程中是不能使用同步阻塞方法的，否则会阻塞整个线程。这里应该使用aiofiles库进行异步文件io。


aiohttp还支持以流的形式上传文件和使用websockets。

另外，aiohttp还支持使用proxy反向代理。

具体可以查看文档

